{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mrityunjay kumar # \n",
    "# TensorFlow 7 (Neural Network - Classification Example + Saver_ to save the Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0129 11:04:41.975889 4676462016 deprecation.py:323] From <ipython-input-3-515adfd38853>:33: read_data_sets (from input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as: tensorflow_datasets.load('mnist')\n",
      "W0129 11:04:41.983748 4676462016 deprecation.py:323] From /Users/mrityunjay/Desktop/Mrityunjay_Kumar/Tensor_NN/input_data.py:297: _maybe_download (from input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0129 11:04:41.989925 4676462016 deprecation.py:323] From /Users/mrityunjay/Desktop/Mrityunjay_Kumar/Tensor_NN/input_data.py:299: _extract_images (from input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0129 11:04:42.352662 4676462016 deprecation.py:323] From /Users/mrityunjay/Desktop/Mrityunjay_Kumar/Tensor_NN/input_data.py:304: _extract_labels (from input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0129 11:04:42.355726 4676462016 deprecation.py:323] From /Users/mrityunjay/Desktop/Mrityunjay_Kumar/Tensor_NN/input_data.py:112: _dense_to_one_hot (from input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "W0129 11:04:42.445199 4676462016 deprecation.py:323] From /Users/mrityunjay/Desktop/Mrityunjay_Kumar/Tensor_NN/input_data.py:328: _DataSet.__init__ (from input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/_DataSet.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0129 11:04:43.754949 4676462016 deprecation.py:323] From /anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "W0129 11:04:44.677636 4676462016 deprecation.py:323] From <ipython-input-3-515adfd38853>:27: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.math.argmax` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0992\n",
      "0.7408\n",
      "0.8043\n",
      "0.8312\n",
      "0.8435\n",
      "0.852\n",
      "0.8598\n",
      "0.8646\n",
      "0.8672\n",
      "0.8722\n",
      "0.8757\n",
      "0.8785\n",
      "0.8798\n",
      "0.8844\n",
      "0.8849\n",
      "0.8866\n",
      "0.8872\n",
      "0.8892\n",
      "0.8894\n",
      "0.8916\n",
      "0.8923\n",
      "0.8934\n",
      "0.8948\n",
      "0.8957\n",
      "0.8955\n",
      "0.8963\n",
      "0.897\n",
      "0.8992\n",
      "0.8983\n",
      "0.8976\n",
      "0.8988\n",
      "0.9004\n",
      "0.8992\n",
      "0.9019\n",
      "0.9018\n",
      "0.902\n",
      "0.9017\n",
      "0.9035\n",
      "0.9029\n",
      "0.9043\n",
      "0.9052\n",
      "0.9032\n",
      "0.9055\n",
      "0.9054\n",
      "0.905\n",
      "0.9069\n",
      "0.9064\n",
      "0.9067\n",
      "0.9071\n",
      "0.9078\n",
      "0.9084\n",
      "0.9088\n",
      "0.9072\n",
      "0.9086\n",
      "0.9087\n",
      "0.9099\n",
      "0.9094\n",
      "0.9083\n",
      "0.9096\n",
      "0.9096\n",
      "0.9111\n",
      "0.9107\n",
      "0.9091\n",
      "0.9109\n",
      "0.9112\n",
      "0.9113\n",
      "0.9123\n",
      "0.9112\n",
      "0.9128\n",
      "0.9128\n",
      "0.9122\n",
      "0.9128\n",
      "0.9134\n",
      "0.9123\n",
      "0.9126\n",
      "0.9134\n",
      "0.9132\n",
      "0.9137\n",
      "0.9118\n",
      "0.9135\n",
      "0.9137\n",
      "0.914\n",
      "0.9149\n",
      "0.9139\n",
      "0.9145\n",
      "0.9148\n",
      "0.9141\n",
      "0.9146\n",
      "0.9147\n",
      "0.9154\n",
      "0.9153\n",
      "0.9161\n",
      "0.9158\n",
      "0.9154\n",
      "0.9149\n",
      "0.9162\n",
      "0.9147\n",
      "0.9161\n",
      "0.9172\n",
      "0.9166\n",
      "0.9172\n",
      "0.9163\n",
      "0.9168\n",
      "0.9171\n",
      "0.9174\n",
      "0.9166\n",
      "0.9181\n",
      "0.9182\n",
      "0.9174\n",
      "0.9179\n",
      "0.9178\n",
      "0.9174\n",
      "0.9182\n",
      "0.9177\n",
      "0.9174\n",
      "0.9169\n",
      "0.9181\n",
      "0.9165\n",
      "0.9181\n",
      "0.9181\n",
      "0.9181\n",
      "0.9183\n",
      "0.918\n",
      "0.9179\n",
      "0.9186\n",
      "0.9182\n",
      "0.9185\n",
      "0.9185\n",
      "0.9181\n",
      "0.9189\n",
      "0.9189\n",
      "0.919\n",
      "0.9187\n",
      "0.9195\n",
      "0.9186\n",
      "0.919\n",
      "0.9182\n",
      "0.9189\n",
      "0.9186\n",
      "0.9189\n",
      "0.9192\n",
      "0.9193\n",
      "0.9197\n",
      "0.9195\n",
      "0.9195\n",
      "0.9197\n",
      "0.9196\n",
      "0.9192\n",
      "0.9191\n",
      "0.9195\n",
      "0.9198\n",
      "0.9195\n",
      "0.9199\n",
      "0.9193\n",
      "0.9198\n",
      "0.9199\n",
      "0.9206\n",
      "0.9196\n",
      "0.919\n",
      "0.9192\n",
      "0.9199\n",
      "0.9201\n",
      "0.9194\n",
      "0.9196\n",
      "0.9196\n",
      "0.9205\n",
      "0.9211\n",
      "0.9207\n",
      "0.9201\n",
      "0.92\n",
      "0.9207\n",
      "0.9207\n",
      "0.921\n",
      "0.9217\n",
      "0.9207\n",
      "0.9205\n",
      "0.921\n",
      "0.9204\n",
      "0.9205\n",
      "0.9203\n",
      "0.9207\n",
      "0.9203\n",
      "0.9205\n",
      "0.9207\n",
      "0.9222\n",
      "0.9212\n",
      "0.9214\n",
      "0.9218\n",
      "0.9211\n",
      "0.9215\n",
      "0.9207\n",
      "0.9211\n",
      "0.9211\n",
      "0.9215\n",
      "0.9219\n",
      "0.9208\n",
      "0.9215\n",
      "0.9222\n",
      "0.9215\n",
      "0.9218\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#defining the structure of a layer\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "def add_layer(inputs,in_size,out_size,activation_function=None):\n",
    "    #inputs from previous layer\n",
    "    #in_size: size(number of neurons) of previous layer\n",
    "    #out_size: size(number of neurons) of this layer\n",
    "    #activation_function: used in this layer\n",
    "    Weights = tf.compat.v1.Variable(tf.compat.v1.random_normal([in_size,out_size]),name='W')\n",
    "    biases = tf.compat.v1.Variable(tf.compat.v1.zeros([1,out_size])+0.1,name='b')\n",
    "    logit = tf.compat.v1.matmul(inputs,Weights) + biases\n",
    "    if activation_function is None:\n",
    "        output = logit\n",
    "    else:\n",
    "        output = activation_function(logit,)\n",
    "    return output\n",
    "\n",
    "#calculating accuracy\n",
    "def compute_accuracy(xtest,ytest):\n",
    "    global predicted\n",
    "    ypred = sess.run(predicted,feed_dict={xs:xtest})\n",
    "    correct_pred = tf.compat.v1.equal(tf.compat.v1.arg_max(ypred,1),tf.compat.v1.arg_max(ytest,1))\n",
    "    accuracy = tf.compat.v1.reduce_mean(tf.compat.v1.cast(correct_pred,tf.compat.v1.float32))\n",
    "    result = sess.run(accuracy,feed_dict={xs:xtest,ys:ytest})\n",
    "    return result\n",
    "\n",
    "import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST/\", one_hot=True)\n",
    "\n",
    "#define placeholders for input to networks\n",
    "xs = tf.compat.v1.placeholder(tf.compat.v1.float32,[None,784])  #28x28=784\n",
    "ys = tf.compat.v1.placeholder(tf.compat.v1.float32,[None,10])   # 10 output classes\n",
    "\n",
    "#just adding one layer i.e. directly the output layer\n",
    "#adding one layer\n",
    "predicted = add_layer(xs,784,10,tf.compat.v1.nn.softmax)\n",
    "\n",
    "#y_hat_softmax = tf.nn.softmax(y_hat)  , yhat is logit\n",
    "#total_loss = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(y_hat_softmax), [1]))\n",
    "#or\n",
    "#total_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_hat, y_true))\n",
    "\n",
    "cross_entropy = tf.compat.v1.reduce_mean(-tf.compat.v1.reduce_sum(ys*tf.compat.v1.log(predicted),reduction_indices=[1]))\n",
    "training = tf.compat.v1.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "sess = tf.compat.v1.Session()\n",
    "\n",
    "#important step\n",
    "sess.run(tf.compat.v1.initialize_all_variables())\n",
    "\n",
    "\n",
    "for i in range(20000):\n",
    "    batch_xs,batch_ys = mnist.train.next_batch(1000)\n",
    "    sess.run(training,feed_dict={xs:batch_xs,ys:batch_ys})\n",
    "    if i%100==0:\n",
    "        print ( compute_accuracy(mnist.test.images,mnist.test.labels))\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saver : how to save your neural network\n",
    "#save to ckpt file type\n",
    "#remember the dtype, shape and name while restoring\n",
    "\n",
    "#next cell : saving weights and biases in ckpt file\n",
    "#next to next cell : restoring the weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST/t10k-labels-idx1-ubyte.gz\n",
      "saved to path : my_net/nnet_classification.ckpt\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#defining the structure of a layer\n",
    "\n",
    "def add_layer(inputs,in_size,out_size,activation_function=None):\n",
    "    #inputs from previous layer\n",
    "    #in_size: size(number of neurons) of previous layer\n",
    "    #out_size: size(number of neurons) of this layer\n",
    "    #activation_function: used in this layer\n",
    "    Weights = tf.compat.v1.Variable(tf.compat.v1.random_normal([in_size,out_size]),dtype=tf.compat.v1.float32,name='W')\n",
    "    biases = tf.compat.v1.Variable(tf.compat.v1.zeros([1,out_size])+0.1,dtype=tf.compat.v1.float32,name='b')\n",
    "    logit = tf.compat.v1.matmul(inputs,Weights) + biases\n",
    "    if activation_function is None:\n",
    "        output = logit\n",
    "    else:\n",
    "        output = activation_function(logit,)\n",
    "    return output\n",
    "\n",
    "#calculating accuracy\n",
    "def compute_accuracy(xtest,ytest):\n",
    "    global predicted\n",
    "    ypred = sess.run(predicted,feed_dict={xs:xtest})\n",
    "    correct_pred = tf.compat.v1.equal(tf.compat.v1.arg_max(ypred,1),tf.compat.v1.arg_max(ytest,1))\n",
    "    accuracy = tf.compat.v1.reduce_mean(tf.compat.v1.cast(correct_pred,tf.compat.v1.float32))\n",
    "    result = sess.run(accuracy,feed_dict={xs:xtest,ys:ytest})\n",
    "    return result\n",
    "\n",
    "import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST/\", one_hot=True)\n",
    "\n",
    "#define placeholders for input to networks\n",
    "xs = tf.compat.v1.placeholder(tf.compat.v1.float32,[None,784])  #28x28=784\n",
    "ys = tf.compat.v1.placeholder(tf.compat.v1.float32,[None,10])   # 10 output classes\n",
    "\n",
    "#just adding one layer i.e. directly the output layer\n",
    "#adding one layer\n",
    "predicted = add_layer(xs,784,10,tf.compat.v1.nn.softmax)\n",
    "\n",
    "#y_hat_softmax = tf.nn.softmax(y_hat)  , yhat is logit\n",
    "#total_loss = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(y_hat_softmax), [1]))\n",
    "#or\n",
    "#total_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_hat, y_true))\n",
    "\n",
    "cross_entropy = tf.compat.v1.reduce_mean(-tf.compat.v1.reduce_sum(ys*tf.compat.v1.log(predicted),reduction_indices=[1]))\n",
    "training = tf.compat.v1.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "saver = tf.compat.v1.train.Saver() \n",
    "\n",
    "sess = tf.compat.v1.Session()\n",
    "\n",
    "#important step\n",
    "sess.run(tf.compat.v1.initialize_all_variables())\n",
    "savepath = saver.save(sess,'my_net/nnet_classification.ckpt')\n",
    "print (\"saved to path : \"+savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0129 11:04:13.001788 4676462016 deprecation.py:323] From /anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weights': array([[ 1.3773408 ,  0.3468901 ,  0.85125965, ..., -0.21412188,\n",
      "         0.20554833,  1.2731993 ],\n",
      "       [ 0.97289777, -0.23650147, -0.74497116, ..., -2.2251682 ,\n",
      "         0.21947522,  1.3002759 ],\n",
      "       [-0.79577065, -0.41987392,  1.203742  , ...,  0.38149637,\n",
      "         0.23211662,  0.5605953 ],\n",
      "       ...,\n",
      "       [-1.5704225 , -0.47240752, -0.48512638, ...,  0.67337   ,\n",
      "        -0.34762198,  0.75444585],\n",
      "       [ 0.9728448 ,  1.0715126 , -0.29215524, ...,  0.42635995,\n",
      "        -1.1419097 ,  2.4903333 ],\n",
      "       [-0.08233507,  1.2200274 ,  0.85224915, ...,  0.04835549,\n",
      "        -0.46284536,  0.23310702]], dtype=float32), 'biases': array([[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "#restore will need to redefine those weights\n",
    "#no need to run init\n",
    "\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def restore_layer_variable(in_size,out_size):\n",
    "    #inputs from previous layer\n",
    "    #in_size: size(number of neurons) of previous layer\n",
    "    #out_size: size(number of neurons) of this layer\n",
    "    #activation_function: used in this layer\n",
    "    Weights = tf.compat.v1.Variable(tf.compat.v1.random_normal([in_size,out_size]),dtype=tf.compat.v1.float32,name='W')\n",
    "    biases = tf.compat.v1.Variable(tf.compat.v1.zeros([1,out_size])+0.1,dtype=tf.compat.v1.float32,name='b')\n",
    "    saver = tf.compat.v1.train.Saver()\n",
    "    saver.restore(sess,'my_net/nnet_classification.ckpt')\n",
    "    return {'weights':sess.run(Weights),'biases':sess.run(biases)}\n",
    "    \n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    l1 = restore_layer_variable(784,10)\n",
    "    print (l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9217\n",
      "0.9208\n",
      "0.9219\n",
      "0.9224\n",
      "0.922\n",
      "0.9216\n",
      "0.9209\n",
      "0.9213\n",
      "0.9212\n",
      "0.9224\n",
      "0.9215\n",
      "0.922\n",
      "0.9216\n",
      "0.9217\n",
      "0.9228\n",
      "0.9221\n",
      "0.9224\n",
      "0.9221\n",
      "0.9221\n",
      "0.9218\n",
      "0.9221\n",
      "0.9223\n",
      "0.9221\n",
      "0.9217\n",
      "0.9219\n",
      "0.9217\n",
      "0.9223\n",
      "0.9212\n",
      "0.9222\n",
      "0.9224\n",
      "0.9219\n",
      "0.9227\n",
      "0.9221\n",
      "0.9225\n",
      "0.9232\n",
      "0.9224\n",
      "0.9219\n",
      "0.9226\n",
      "0.9226\n",
      "0.9236\n",
      "0.922\n",
      "0.9224\n",
      "0.923\n",
      "0.9228\n",
      "0.9229\n",
      "0.9225\n",
      "0.9221\n",
      "0.9228\n",
      "0.9225\n",
      "0.9224\n",
      "0.9222\n",
      "0.9222\n",
      "0.923\n",
      "0.9222\n",
      "0.9231\n",
      "0.9219\n",
      "0.9232\n",
      "0.923\n",
      "0.9227\n",
      "0.9218\n",
      "0.923\n",
      "0.9229\n",
      "0.9234\n",
      "0.9227\n",
      "0.9225\n",
      "0.9229\n",
      "0.9229\n",
      "0.9223\n",
      "0.9221\n",
      "0.9235\n",
      "0.9222\n",
      "0.9223\n",
      "0.9223\n",
      "0.9237\n",
      "0.9222\n",
      "0.9226\n",
      "0.9222\n",
      "0.9234\n",
      "0.9235\n",
      "0.9238\n",
      "0.9237\n",
      "0.922\n",
      "0.9235\n",
      "0.9232\n",
      "0.9227\n",
      "0.9236\n",
      "0.9247\n",
      "0.9227\n",
      "0.9238\n",
      "0.9232\n",
      "0.9225\n",
      "0.9236\n",
      "0.923\n",
      "0.9235\n",
      "0.9241\n",
      "0.9222\n",
      "0.9233\n",
      "0.9233\n",
      "0.9225\n",
      "0.9229\n",
      "0.9219\n",
      "0.9239\n",
      "0.9246\n",
      "0.9224\n",
      "0.9242\n",
      "0.9243\n",
      "0.9244\n",
      "0.9228\n",
      "0.9226\n",
      "0.9234\n",
      "0.9235\n",
      "0.9235\n",
      "0.9234\n",
      "0.9223\n",
      "0.9232\n",
      "0.9228\n",
      "0.9232\n",
      "0.9231\n",
      "0.9239\n",
      "0.9234\n",
      "0.9239\n",
      "0.922\n",
      "0.9241\n",
      "0.9226\n",
      "0.9239\n",
      "0.9226\n",
      "0.9229\n",
      "0.9249\n",
      "0.9228\n",
      "0.9246\n",
      "0.9237\n",
      "0.9245\n",
      "0.9242\n",
      "0.9243\n",
      "0.9244\n",
      "0.9242\n",
      "0.9231\n",
      "0.9237\n",
      "0.9237\n",
      "0.9237\n",
      "0.9233\n",
      "0.9231\n",
      "0.9242\n",
      "0.9235\n",
      "0.9234\n",
      "0.9237\n",
      "0.9231\n",
      "0.9232\n",
      "0.9224\n",
      "0.9242\n",
      "0.9247\n",
      "0.924\n",
      "0.9239\n",
      "0.9234\n",
      "0.9243\n",
      "0.9239\n",
      "0.9233\n",
      "0.9232\n",
      "0.9229\n",
      "0.924\n",
      "0.924\n",
      "0.9245\n",
      "0.9249\n",
      "0.9244\n",
      "0.9245\n",
      "0.9237\n",
      "0.9233\n",
      "0.9237\n",
      "0.9248\n",
      "0.9242\n",
      "0.924\n",
      "0.9239\n",
      "0.9247\n",
      "0.924\n",
      "0.9246\n",
      "0.9245\n",
      "0.9249\n",
      "0.9244\n",
      "0.9237\n",
      "0.9242\n",
      "0.9244\n",
      "0.9246\n",
      "0.9245\n",
      "0.9242\n",
      "0.9249\n",
      "0.9234\n",
      "0.9254\n",
      "0.9247\n",
      "0.9241\n",
      "0.925\n",
      "0.9241\n",
      "0.9242\n",
      "0.9247\n",
      "0.9239\n",
      "0.9242\n",
      "0.9244\n",
      "0.9251\n",
      "0.9235\n",
      "0.9242\n",
      "0.9244\n"
     ]
    }
   ],
   "source": [
    "for i in range(20000):\n",
    "    batch_xs,batch_ys = mnist.train.next_batch(1000)\n",
    "    sess.run(training,feed_dict={xs:batch_xs,ys:batch_ys})\n",
    "    if i%100==0:\n",
    "        print (compute_accuracy(mnist.test.images,mnist.test.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc=mnist.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcd,cde=abc.next_batch(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cde.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.compat.v1.argmax(cde,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 6, 7, 2, 0, 8, 9, 3, 7, 6, 7, 7, 3, 6, 4, 2, 1, 1, 1, 7,\n",
       "       2, 4, 1, 2, 3, 2, 0, 8, 6, 7, 9, 3, 5, 2, 4, 4, 8, 4, 3, 6, 4, 1,\n",
       "       8, 5, 4, 0, 3, 0, 2, 8, 4, 4, 0, 4, 2, 2, 5, 8, 4, 2, 4, 3, 3, 8,\n",
       "       7, 6, 3, 2, 1, 7, 7, 0, 8, 2, 0, 2, 6, 3, 5, 8, 2, 6, 3, 9, 7, 0,\n",
       "       0, 9, 9, 3, 1, 5, 8, 3, 0, 2, 4, 8, 3, 0, 1, 4, 3, 0, 0, 5, 2, 5,\n",
       "       1, 1, 5, 1, 2, 0, 4, 7, 9, 0, 2, 0, 2, 1, 0, 0, 2, 1, 8, 1, 4, 1,\n",
       "       6, 3, 9, 0, 3, 0, 3, 0, 5, 0, 2, 3, 8, 6, 8, 7, 4, 1, 0, 6, 8, 1,\n",
       "       9, 5, 4, 1, 1, 0, 3, 2, 2, 2, 9, 4, 4, 8, 7, 1, 6, 8, 3, 5, 7, 1,\n",
       "       5, 0, 0, 2, 1, 6, 3, 5, 4, 3, 9, 4, 4, 4, 4, 2, 6, 0, 7, 7, 2, 2,\n",
       "       3, 0, 9, 1, 5, 6, 9, 8, 9, 6, 6, 7, 3, 5, 9, 7, 7, 2, 9, 4, 6, 1,\n",
       "       6, 4, 6, 6, 9, 7, 2, 1, 7, 2, 1, 8, 8, 5, 4, 4, 4, 5, 1, 2, 7, 1,\n",
       "       6, 0, 3, 1, 6, 5, 2, 2, 4, 1, 7, 3, 3, 3, 9, 1, 7, 9, 3, 5, 1, 1,\n",
       "       4, 4, 7, 8, 6, 0, 7, 2, 9, 2, 8, 8, 3, 6, 2, 5, 2, 8, 8, 1, 0, 2,\n",
       "       4, 6, 8, 6, 1, 3, 7, 6, 2, 2, 2, 9, 4, 1, 7, 0, 1, 1, 9, 1, 2, 8,\n",
       "       4, 7, 9, 7, 7, 8, 6, 2, 9, 1, 2, 5, 3, 2, 6, 2, 7, 0, 0, 8, 3, 3,\n",
       "       2, 1, 8, 1, 3, 1, 1, 2, 6, 5, 9, 0, 4, 6, 6, 2, 3, 7, 2, 2, 3, 8,\n",
       "       9, 5, 0, 1, 3, 3, 2, 8, 4, 9, 6, 8, 9, 0, 6, 4, 1, 3, 9, 5, 0, 5,\n",
       "       3, 5, 5, 1, 6, 2, 1, 9, 9, 2, 8, 6, 7, 9, 2, 6, 2, 0, 2, 1, 5, 8,\n",
       "       0, 4, 5, 3, 7, 5, 5, 5, 5, 0, 5, 1, 2, 1, 0, 0, 0, 6, 3, 6, 8, 3,\n",
       "       3, 8, 1, 4, 4, 7, 2, 0, 7, 6, 2, 3, 8, 3, 6, 3, 1, 1, 2, 4, 3, 1,\n",
       "       9, 7, 9, 9, 0, 3, 1, 9, 4, 7, 6, 3, 3, 5, 1, 4, 3, 8, 8, 2, 0, 5,\n",
       "       9, 7, 5, 6, 5, 1, 4, 4, 5, 1, 0, 5, 9, 1, 8, 3, 7, 1, 7, 2, 1, 6,\n",
       "       3, 5, 8, 1, 0, 1, 5, 1, 2, 1, 9, 1, 6, 1, 1, 9])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.compat.v1.Session().run(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 1., 6., 7., 2., 0., 8., 9., 3., 7., 6., 7., 7., 3., 6., 4.,\n",
       "       2., 1., 1., 1., 7., 2., 4., 1., 2., 3., 2., 0., 8., 6., 7., 9., 3.,\n",
       "       5., 2., 4., 4., 8., 4., 3., 6., 4., 1., 8., 5., 4., 0., 3., 0., 2.,\n",
       "       8., 4., 4., 0., 4., 2., 2., 5., 8., 4., 2., 4., 3., 3., 8., 7., 6.,\n",
       "       3., 2., 1., 7., 7., 0., 8., 2., 0., 2., 6., 3., 5., 8., 2., 6., 3.,\n",
       "       9., 7., 0., 0., 9., 9., 3., 1., 5., 8., 3., 0., 2., 4., 8., 3., 0.,\n",
       "       1., 4., 3., 0., 0., 5., 2., 5., 1., 1., 5., 1., 2., 0., 4., 7., 9.,\n",
       "       0., 2., 0., 2., 1., 0., 0., 2., 1., 8., 1., 4., 1., 6., 3., 9., 0.,\n",
       "       3., 0., 3., 0., 5., 0., 2., 3., 8., 6., 8., 7., 4., 1., 0., 6., 8.,\n",
       "       1., 9., 5., 4., 1., 1., 0., 3., 2., 2., 2., 9., 4., 4., 8., 7., 1.,\n",
       "       6., 8., 3., 5., 7., 1., 5., 0., 0., 2., 1., 6., 3., 5., 4., 3., 9.,\n",
       "       4., 4., 4., 4., 2., 6., 0., 7., 7., 2., 2., 3., 0., 9., 1., 5., 6.,\n",
       "       9., 8., 9., 6., 6., 7., 3., 5., 9., 7., 7., 2., 9., 4., 6., 1., 6.,\n",
       "       4., 6., 6., 9., 7., 2., 1., 7., 2., 1., 8., 8., 5., 4., 4., 4., 5.,\n",
       "       1., 2., 7., 1., 6., 0., 3., 1., 6., 5., 2., 2., 4., 1., 7., 3., 3.,\n",
       "       3., 9., 1., 7., 9., 3., 5., 1., 1., 4., 4., 7., 8., 6., 0., 7., 2.,\n",
       "       9., 2., 8., 8., 3., 6., 2., 5., 2., 8., 8., 1., 0., 2., 4., 6., 8.,\n",
       "       6., 1., 3., 7., 6., 2., 2., 2., 9., 4., 1., 7., 0., 1., 1., 9., 1.,\n",
       "       2., 8., 4., 7., 9., 7., 7., 8., 6., 2., 9., 1., 2., 5., 3., 2., 6.,\n",
       "       2., 7., 0., 0., 8., 3., 3., 2., 1., 8., 1., 3., 1., 1., 2., 6., 5.,\n",
       "       9., 0., 4., 6., 6., 2., 3., 7., 2., 2., 3., 8., 9., 5., 0., 1., 3.,\n",
       "       3., 2., 8., 4., 9., 6., 8., 9., 0., 6., 4., 1., 3., 9., 5., 0., 5.,\n",
       "       3., 5., 5., 1., 6., 2., 1., 9., 9., 2., 8., 6., 7., 9., 2., 6., 2.,\n",
       "       0., 2., 1., 5., 8., 0., 4., 5., 3., 7., 5., 5., 5., 5., 0., 5., 1.,\n",
       "       2., 1., 0., 0., 0., 6., 3., 6., 8., 3., 3., 8., 1., 4., 4., 7., 2.,\n",
       "       0., 7., 6., 2., 3., 8., 3., 6., 3., 1., 1., 2., 4., 3., 1., 9., 7.,\n",
       "       9., 9., 0., 3., 1., 9., 4., 7., 6., 3., 3., 5., 1., 4., 3., 8., 8.,\n",
       "       2., 0., 5., 9., 7., 5., 6., 5., 1., 4., 4., 5., 1., 0., 5., 9., 1.,\n",
       "       8., 3., 7., 1., 7., 2., 1., 6., 3., 5., 8., 1., 0., 1., 5., 1., 2.,\n",
       "       1., 9., 1., 6., 1., 1., 9.], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.compat.v1.Session().run(tf.compat.v1.cast(a,tf.compat.v1.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
