{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Restore\n",
    "In this post we are going to talk about how to save the parameters into the disk and restore the saved parameters from the disk. The savable/restorable paramters of the network are __Variables__ (i.e. weights and biases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TLDR: \n",
    "\n",
    "To save and restore your variables, all you need to do is to call the `tf.train.Saver()` at the end of you graph.\n",
    "\n",
    "```python\n",
    "# create the graph\n",
    "X = tf.placeholder(..)\n",
    "Y = tf.placeholder(..)\n",
    "w = tf.get_variable(..)\n",
    "b = tf.get_variable(..)\n",
    "...\n",
    "loss = tf.losses.mean_squared_error(..)\n",
    "optimizer = tf.train.AdamOptimizer(..).minimize(loss)\n",
    "...\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "```\n",
    "\n",
    "\n",
    "__In the train mode__, in the session we will initialize the variables and run our network. At the end of training, we will save the variables using `saver.save()`:\n",
    "\n",
    "```python\n",
    "# TRAIN\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # train our model\n",
    "    for step in range(steps):\n",
    "        sess.run(optimizer)\n",
    "        ...\n",
    "    saved_path = saver.save(sess, './my-model', global_step=step)\n",
    "```\n",
    "\n",
    "This will create 3 files (`data`, `index`, `meta`) with a suffix of the step you saved your model.\n",
    "\n",
    "__In the test mode__, in the session we will restore the variables using `saver.restore()` and validate or test our model.\n",
    "\n",
    "```python\n",
    "# TEST\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './my-model')\n",
    "    ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import the required libraries:\n",
    "\n",
    "We will start with importing the required Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import tensorflow as tf\n",
    "import os\n",
    "tf.compat.v1.disable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()   # To clear the defined variables and operations of the previous cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Save and Restore Two Variables:\n",
    "### 1.1 Save:\n",
    "We will start with saving and restoring two variables in TensorFlow. We will create a graph with two variables. Let's create two variables `a = [3 3]` and `b = [5 5 5]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variables a and b\n",
    "a = tf.compat.v1.get_variable(\"A\", initializer=tf.compat.v1.constant(3, shape=[2]))\n",
    "b = tf.compat.v1.get_variable(\"B\", initializer=tf.compat.v1.constant(5, shape=[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the __lower__case letter as python name and __UPPER__case letter as TensorFlow name. It will be important when we want to import the graph in restoring the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Recall from the [Tensor Types Tutorial](https://github.com/easy-tensorflow/easy-tensorflow/blob/master/1_TensorFlow_Basics/Tutorials/2_Tensor_Types.ipynb):__ Variables need to be initialized before being used. To do so, we have to invoke a __variable initializer operation__ and run the operation on the session. This is the easiest way to initialize variables which initializes all variables at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize all of the variables\n",
    "init_op = tf.compat.v1.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, on the session, we can initialize the variables and run the to see the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a =  [3 3]\n",
      "b =  [5 5 5]\n"
     ]
    }
   ],
   "source": [
    "# run the sessiontf.reset_default_graph()   # To clear the defined variables and operations of the previous cell\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    # initialize all of the variables in the session\n",
    "    sess.run(init_op)\n",
    "    # run the session to get the value of the variable\n",
    "    a_out, b_out = sess.run([a, b])\n",
    "    print('a = ', a_out)\n",
    "    print('b = ', b_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Important Note:__ All of the variables exist in the scope of the session. So, after the session is closed, we will loose the variable. \n",
    "\n",
    "In order to save the variable, we will call the saver function using `tf.train.Saver()` in our graph. This function will find all the variables in the graph. We can see the list of all variables in `_var_list`. Let's create a `saver` object and take a look at the `_var_list` in the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Var 0: <tf.Variable 'A:0' shape=(2,) dtype=int32>\n",
      "Var 1: <tf.Variable 'B:0' shape=(3,) dtype=int32>\n"
     ]
    }
   ],
   "source": [
    "# create saver object\n",
    "saver = tf.compat.v1.train.Saver()\n",
    "for i, var in enumerate(saver._var_list):\n",
    "    print('Var {}: {}'.format(i, var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our graph consists of two variables that listed above.\n",
    "\n",
    "__Important Note__: Notice the `:0` at the end of the variable name. For more about tensor naming check [here](https://stackoverflow.com/questions/36150834/how-does-tensorflow-name-tensors).\n",
    "\n",
    "Now that the saver object is created in the graph, in the session, we can call the `saver.save()` function to save the variables in the disk. We have to pass the created session (`sess`) and the path to the file that we want to save the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved in ./saved_variable\n"
     ]
    }
   ],
   "source": [
    "# run the session\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    # initialize all of the variables in the session\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # save the variable in the disk\n",
    "    saved_path = saver.save(sess, './saved_variable')\n",
    "    print('model saved in {}'.format(saved_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you check your working directory, you will notice that 3  new files have been created with the name `saved_variable` in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_variable.data-00000-of-00001\n",
      "saved_variable.meta\n",
      "saved_variable.index\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir('.'):\n",
    "    if 'saved_variable' in file:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__.data:__ Contains variable values\n",
    "\n",
    "__.meta:__ Contains graph structure\n",
    "\n",
    "__.index:__ Identifies checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Restore:\n",
    "Now that all the things that you need is saved in the disk, you can load your saved variables in the session using `saver.restore()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0128 12:18:21.451606 4630574528 deprecation.py:323] From /anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a =  [3 3]\n",
      "b =  [5 5 5]\n"
     ]
    }
   ],
   "source": [
    "# run the session\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    # restore the saved vairable\n",
    "    saver.restore(sess, './saved_variable')\n",
    "    # print the loaded variable\n",
    "    a_out, b_out = sess.run([a, b])\n",
    "    print('a = ', a_out)\n",
    "    print('b = ', b_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this time we did not initialize the variables in our session. Instead, we restored them from the disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Important Note:__ In order to restore the parameters, the graph should be defined. Since we defined the graph in top, we didn't have a problem restoring the parameters. But what happens if we have not loaded the graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Session graph is empty.  Add operations to the graph before calling run().\n"
     ]
    }
   ],
   "source": [
    "# delete the current graph\n",
    "tf.compat.v1.reset_default_graph()\n",
    "try:\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        # restore the saved vairable\n",
    "        saver.restore(sess, './saved_variable')\n",
    "        # print the loaded variable\n",
    "        a_out, b_out = sess.run([a, b])\n",
    "        print('a = ', a_out)\n",
    "        print('b = ', b_out)\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the graph in two ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1. Define the graph from scratch and then run the session:\n",
    "This way is simple if you have your graph. So, what you can  do is to create the graph and then restore your variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a =  [3 3]\n",
      "b =  [5 5 5]\n"
     ]
    }
   ],
   "source": [
    "# delete the current graph\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# create a new graph\n",
    "# create variables a and b\n",
    "a = tf.compat.v1.get_variable(\"A\", initializer=tf.compat.v1.constant(3, shape=[2]))\n",
    "b = tf.compat.v1.get_variable(\"B\", initializer=tf.compat.v1.constant(5, shape=[3]))\n",
    "\n",
    "# initialize all of the variables\n",
    "init_op = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "# create saver object\n",
    "saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "# run the session\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    # restore the saved vairable\n",
    "    saver.restore(sess, './saved_variable')\n",
    "    # print the loaded variable\n",
    "    a_out, b_out = sess.run([a, b])\n",
    "    print('a = ', a_out)\n",
    "    print('b = ', b_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that the graph should be exactly like the one that you saved. ButwWhat if we do not know the exact graph and we are using someone else's pre-trained model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2. Restore the graph from `.meta` file.\n",
    "\n",
    "When we save the variables, it creates a `.meta` file. This file contains the graph structure. Therefore, we can import the meta graph using `tf.train.import_meta_graph()` and restore the values of the graph. Let's import the graph and see all tensors in the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Const\n",
      "A\n",
      "A/IsInitialized/VarIsInitializedOp\n",
      "A/Assign\n",
      "A/Read/ReadVariableOp\n",
      "Const_1\n",
      "B\n",
      "B/IsInitialized/VarIsInitializedOp\n",
      "B/Assign\n",
      "B/Read/ReadVariableOp\n",
      "init\n",
      "init_1\n",
      "save/filename/input\n",
      "save/filename\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Identity\n",
      "save/AssignVariableOp\n",
      "save/Identity_1\n",
      "save/AssignVariableOp_1\n",
      "save/restore_all\n"
     ]
    }
   ],
   "source": [
    "# delete the current graph\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# import the graph from the file\n",
    "imported_graph = tf.compat.v1.train.import_meta_graph('saved_variable.meta')\n",
    "\n",
    "# list all the tensors in the graph\n",
    "for tensor in tf.compat.v1.get_default_graph().get_operations():\n",
    "    print (tensor.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you recall from section 1.1, we defined the python names with __lower__case letters and in TensorFlow names with __UPPER__case letters. You can see that what we have here are the __UPPER__case letter variables. It means that `tf.train.Saver()` saves the variables with the TensorFlow name. Now that we have the imported graph, and we know that we are interested in `A` and `B` tensors, we can restore the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a =  [( 10,) ( 44,) ( 47,) (106,) (111,) ( 98,) ( 58,) (108,) (111,) ( 99,)\n",
      " ( 97,) (108,) (104,) (111,) (115,) (116,) ( 47,) (114,) (101,) (112,)\n",
      " (108,) (105,) ( 99,) ( 97,) ( 58,) ( 48,) ( 47,) (116,) ( 97,) (115,)\n",
      " (107,) ( 58,) ( 48,) ( 47,) (100,) (101,) (118,) (105,) ( 99,) (101,)\n",
      " ( 58,) ( 67,) ( 80,) ( 85,) ( 58,) ( 48,) ( 18,) (  9,) (108,) (111,)\n",
      " ( 99,) ( 97,) (108,) (104,) (111,) (115,) (116,) ( 26,) (  1,) ( 65,)\n",
      " ( 32,) (128,) (147,) (239,) (203,) ( 17,) ( 42,) ( 18,) ( 78,) ( 49,)\n",
      " ( 48,) (116,) (101,) (110,) (115,) (111,) (114,) (102,) (108,) (111,)\n",
      " (119,) ( 51,) ( 86,) ( 97,) (114,) ( 69,)]\n",
      "b =  [( 10,) ( 44,) ( 47,) (106,) (111,) ( 98,) ( 58,) (108,) (111,) ( 99,)\n",
      " ( 97,) (108,) (104,) (111,) (115,) (116,) ( 47,) (114,) (101,) (112,)\n",
      " (108,) (105,) ( 99,) ( 97,) ( 58,) ( 48,) ( 47,) (116,) ( 97,) (115,)\n",
      " (107,) ( 58,) ( 48,) ( 47,) (100,) (101,) (118,) (105,) ( 99,) (101,)\n",
      " ( 58,) ( 67,) ( 80,) ( 85,) ( 58,) ( 48,) ( 18,) (  9,) (108,) (111,)\n",
      " ( 99,) ( 97,) (108,) (104,) (111,) (115,) (116,) ( 26,) (  1,) ( 66,)\n",
      " ( 32,) (128,) (147,) (239,) (203,) ( 17,) ( 42,) ( 18,) ( 78,) ( 49,)\n",
      " ( 48,) (116,) (101,) (110,) (115,) (111,) (114,) (102,) (108,) (111,)\n",
      " (119,) ( 51,) ( 86,) ( 97,) (114,) ( 69,)]\n"
     ]
    }
   ],
   "source": [
    "# run the session\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    # restore the saved vairable\n",
    "    imported_graph.restore(sess, './saved_variable')\n",
    "    # print the loaded variable\n",
    "    a_out, b_out = sess.run(['A:0','B:0'])\n",
    "    print('a = ', a_out)\n",
    "    print('b = ', b_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Important Note:__ Notice that in `sess.run()` we provided  the TensorFlow name of the tensors `'A:0'` and `'B:0'` instead of `a` and `b`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Save and Restore Variables of a Sample Linear Model:\n",
    "Now that we have learnt how to save and restore parameters, we can write a simple model and try to save and restore the __weights__ and __biases__ in this network.\n",
    "\n",
    "We will build a simple linear model. If you do not know about the linear model, check our [Linear Classifier Tutorial](https://github.com/easy-tensorflow/easy-tensorflow/tree/master/2_Linear_Classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# delete the current graph\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Data Dimensions\n",
    "img_h = img_w = 28              # MNIST images are 28x28\n",
    "img_size_flat = img_h * img_w   # 28x28=784, the total number of pixels\n",
    "n_classes = 10                  # Number of classes, one class per digit\n",
    "\n",
    "# Load MNIST data\n",
    "import input_data\n",
    "data = input_data.read_data_sets(\"MNIST/\", one_hot=True)\n",
    "\n",
    "# Hyper-parameters\n",
    "learning_rate = 0.001   # The optimization initial learning rate\n",
    "batch_size = 100        # Training batch size\n",
    "num_steps = 1000         # Total number of training steps\n",
    "\n",
    "# Placeholders for inputs (x), outputs(y)\n",
    "x = tf.compat.v1.placeholder(tf.compat.v1.float32, shape=[None, img_size_flat], name='X')\n",
    "y = tf.compat.v1.placeholder(tf.compat.v1.float32, shape=[None, n_classes], name='Y')\n",
    "\n",
    "W = tf.compat.v1.get_variable('W',\n",
    "                    dtype=tf.compat.v1.float32,\n",
    "                    shape=[img_size_flat, n_classes],\n",
    "                    initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.01))\n",
    "b = tf.compat.v1.get_variable('b',\n",
    "                    dtype=tf.compat.v1.float32,\n",
    "                    initializer=tf.compat.v1.constant(0., shape=[n_classes], dtype=tf.compat.v1.float32))\n",
    "\n",
    "# Calculate the output logits as: output_logits = W*x + b\n",
    "output_logits = tf.compat.v1.matmul(x, W) + b\n",
    "# Convert logits to probabilities\n",
    "y_pred = tf.compat.v1.nn.softmax(output_logits)\n",
    "\n",
    "# Define the loss function, optimizer, and accuracy\n",
    "loss = tf.compat.v1.reduce_mean(tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=output_logits), name='loss')\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate, name='Adam-op').minimize(loss)\n",
    "correct_prediction = tf.compat.v1.equal(tf.compat.v1.argmax(output_logits, 1), tf.compat.v1.argmax(y, 1), name='correct_pred')\n",
    "accuracy = tf.compat.v1.reduce_mean(tf.compat.v1.cast(correct_prediction, tf.compat.v1.float32), name='accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of graph, will call the `tf.train.Saver()` to save all the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create saver object\n",
    "saver = tf.compat.v1.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the model and save the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Validation loss: 0.31, Validation accuracy: 91.4%\n",
      "---------------------------------------------------------\n",
      "model saved in ./linear_model\n"
     ]
    }
   ],
   "source": [
    "# run the session\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    for i in range(num_steps):\n",
    "        # Get a batch of training examples and their corresponding labels.\n",
    "        x_batch, y_true_batch = data.train.next_batch(batch_size)\n",
    "        # Put the batch into a dict to be fed into the placeholders\n",
    "        feed_dict_train = {x: x_batch, y: y_true_batch}\n",
    "        sess.run(optimizer, feed_dict=feed_dict_train)\n",
    "\n",
    "    feed_dict_valid = {x: data.validation.images, y: data.validation.labels}\n",
    "    loss_test, acc_test = sess.run([loss, accuracy], feed_dict=feed_dict_valid)\n",
    "    print('---------------------------------------------------------')\n",
    "    print(\"Validation loss: {0:.2f}, Validation accuracy: {1:.01%}\".format(loss_test, acc_test))\n",
    "    print('---------------------------------------------------------')\n",
    "    # save the variable in the disk\n",
    "    saved_path = saver.save(sess, './linear_model')\n",
    "    print('model saved in {}'.format(saved_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the model is saved in `./linear_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_model.data-00000-of-00001\n",
      "linear_model.meta\n",
      "linear_model.index\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir('.'):\n",
    "    if 'linear_model' in file:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's restore the model and pull out the trained variables. at this time, the garph still exists in the memory. So, we can restore it and evaluate the network on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Test loss: 0.32, test accuracy: 91.5%\n",
      "---------------------------------------------------------\n",
      "\n",
      "W =  [( 10,) ( 44,) ( 47,) (106,) (111,) ( 98,) ( 58,) (108,) (111,) ( 99,)\n",
      " ( 97,) (108,) (104,) (111,) (115,) (116,) ( 47,) (114,) (101,) (112,)\n",
      " (108,) (105,) ( 99,) ( 97,) ( 58,) ( 48,) ( 47,) (116,) ( 97,) (115,)\n",
      " (107,) ( 58,) ( 48,) ( 47,) (100,) (101,) (118,) (105,) ( 99,) (101,)\n",
      " ( 58,) ( 67,) ( 80,) ( 85,) ( 58,) ( 48,) ( 18,) (  9,) (108,) (111,)\n",
      " ( 99,) ( 97,) (108,) (104,) (111,) (115,) (116,) ( 26,) (  1,) ( 87,)\n",
      " ( 32,) (128,) (147,) (239,) (203,) ( 17,) ( 42,) ( 18,) ( 78,) ( 49,)\n",
      " ( 48,) (116,) (101,) (110,) (115,) (111,) (114,) (102,) (108,) (111,)\n",
      " (119,) ( 51,) ( 86,) ( 97,) (114,) ( 69,)]\n",
      "b =  [( 10,) ( 44,) ( 47,) (106,) (111,) ( 98,) ( 58,) (108,) (111,) ( 99,)\n",
      " ( 97,) (108,) (104,) (111,) (115,) (116,) ( 47,) (114,) (101,) (112,)\n",
      " (108,) (105,) ( 99,) ( 97,) ( 58,) ( 48,) ( 47,) (116,) ( 97,) (115,)\n",
      " (107,) ( 58,) ( 48,) ( 47,) (100,) (101,) (118,) (105,) ( 99,) (101,)\n",
      " ( 58,) ( 67,) ( 80,) ( 85,) ( 58,) ( 48,) ( 18,) (  9,) (108,) (111,)\n",
      " ( 99,) ( 97,) (108,) (104,) (111,) (115,) (116,) ( 26,) (  1,) ( 98,)\n",
      " ( 32,) (128,) (147,) (239,) (203,) ( 17,) ( 42,) ( 18,) ( 78,) ( 49,)\n",
      " ( 48,) (116,) (101,) (110,) (115,) (111,) (114,) (102,) (108,) (111,)\n",
      " (119,) ( 51,) ( 86,) ( 97,) (114,) ( 69,)]\n"
     ]
    }
   ],
   "source": [
    "# Test the network after training\n",
    "\n",
    "# run the session\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    # restore the saved vairable\n",
    "    saver.restore(sess, './linear_model')\n",
    "    \n",
    "    # Accuracy\n",
    "    feed_dict_test = {x: data.test.images, y: data.test.labels}\n",
    "    loss_test, acc_test = sess.run([loss, accuracy], feed_dict=feed_dict_test)\n",
    "    print('---------------------------------------------------------')\n",
    "    print(\"Test loss: {0:.2f}, test accuracy: {1:.01%}\".format(loss_test, acc_test))\n",
    "    print('---------------------------------------------------------')\n",
    "    print()\n",
    "\n",
    "    \n",
    "    # print the loaded variable\n",
    "    weight, bias = sess.run(['W:0','b:0'])\n",
    "    print('W = ', weight)\n",
    "    print('b = ', bias)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from __Section 1.2__, if we do not have the graph, we can restore the values of the graph using `tf.train.import_meta_graph()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "Y\n",
      "W/Initializer/truncated_normal/shape\n",
      "W/Initializer/truncated_normal/mean\n",
      "W/Initializer/truncated_normal/stddev\n",
      "W/Initializer/truncated_normal/TruncatedNormal\n",
      "W/Initializer/truncated_normal/mul\n",
      "W/Initializer/truncated_normal\n",
      "W\n",
      "W/IsInitialized/VarIsInitializedOp\n",
      "W/Assign\n",
      "W/Read/ReadVariableOp\n",
      "Const\n",
      "b\n",
      "b/IsInitialized/VarIsInitializedOp\n",
      "b/Assign\n",
      "b/Read/ReadVariableOp\n",
      "MatMul/ReadVariableOp\n",
      "MatMul\n",
      "add/ReadVariableOp\n",
      "add\n",
      "Softmax\n",
      "softmax_cross_entropy_with_logits/Rank\n",
      "softmax_cross_entropy_with_logits/Shape\n",
      "softmax_cross_entropy_with_logits/Rank_1\n",
      "softmax_cross_entropy_with_logits/Shape_1\n",
      "softmax_cross_entropy_with_logits/Sub/y\n",
      "softmax_cross_entropy_with_logits/Sub\n",
      "softmax_cross_entropy_with_logits/Slice/begin\n",
      "softmax_cross_entropy_with_logits/Slice/size\n",
      "softmax_cross_entropy_with_logits/Slice\n",
      "softmax_cross_entropy_with_logits/concat/values_0\n",
      "softmax_cross_entropy_with_logits/concat/axis\n",
      "softmax_cross_entropy_with_logits/concat\n",
      "softmax_cross_entropy_with_logits/Reshape\n",
      "softmax_cross_entropy_with_logits/Rank_2\n",
      "softmax_cross_entropy_with_logits/Shape_2\n",
      "softmax_cross_entropy_with_logits/Sub_1/y\n",
      "softmax_cross_entropy_with_logits/Sub_1\n",
      "softmax_cross_entropy_with_logits/Slice_1/begin\n",
      "softmax_cross_entropy_with_logits/Slice_1/size\n",
      "softmax_cross_entropy_with_logits/Slice_1\n",
      "softmax_cross_entropy_with_logits/concat_1/values_0\n",
      "softmax_cross_entropy_with_logits/concat_1/axis\n",
      "softmax_cross_entropy_with_logits/concat_1\n",
      "softmax_cross_entropy_with_logits/Reshape_1\n",
      "softmax_cross_entropy_with_logits\n",
      "softmax_cross_entropy_with_logits/Sub_2/y\n",
      "softmax_cross_entropy_with_logits/Sub_2\n",
      "softmax_cross_entropy_with_logits/Slice_2/begin\n",
      "softmax_cross_entropy_with_logits/Slice_2/size\n",
      "softmax_cross_entropy_with_logits/Slice_2\n",
      "softmax_cross_entropy_with_logits/Reshape_2\n",
      "Const_1\n",
      "loss\n",
      "gradients/Shape\n",
      "gradients/grad_ys_0\n",
      "gradients/Fill\n",
      "gradients/loss_grad/Reshape/shape\n",
      "gradients/loss_grad/Reshape\n",
      "gradients/loss_grad/Shape\n",
      "gradients/loss_grad/Tile\n",
      "gradients/loss_grad/Shape_1\n",
      "gradients/loss_grad/Shape_2\n",
      "gradients/loss_grad/Const\n",
      "gradients/loss_grad/Prod\n",
      "gradients/loss_grad/Const_1\n",
      "gradients/loss_grad/Prod_1\n",
      "gradients/loss_grad/Maximum/y\n",
      "gradients/loss_grad/Maximum\n",
      "gradients/loss_grad/floordiv\n",
      "gradients/loss_grad/Cast\n",
      "gradients/loss_grad/truediv\n",
      "gradients/softmax_cross_entropy_with_logits/Reshape_2_grad/Shape\n",
      "gradients/softmax_cross_entropy_with_logits/Reshape_2_grad/Reshape\n",
      "gradients/zeros_like\n",
      "gradients/softmax_cross_entropy_with_logits_grad/ExpandDims/dim\n",
      "gradients/softmax_cross_entropy_with_logits_grad/ExpandDims\n",
      "gradients/softmax_cross_entropy_with_logits_grad/mul\n",
      "gradients/softmax_cross_entropy_with_logits_grad/LogSoftmax\n",
      "gradients/softmax_cross_entropy_with_logits_grad/Neg\n",
      "gradients/softmax_cross_entropy_with_logits_grad/ExpandDims_1/dim\n",
      "gradients/softmax_cross_entropy_with_logits_grad/ExpandDims_1\n",
      "gradients/softmax_cross_entropy_with_logits_grad/mul_1\n",
      "gradients/softmax_cross_entropy_with_logits_grad/tuple/group_deps\n",
      "gradients/softmax_cross_entropy_with_logits_grad/tuple/control_dependency\n",
      "gradients/softmax_cross_entropy_with_logits_grad/tuple/control_dependency_1\n",
      "gradients/softmax_cross_entropy_with_logits/Reshape_grad/Shape\n",
      "gradients/softmax_cross_entropy_with_logits/Reshape_grad/Reshape\n",
      "gradients/add_grad/Shape\n",
      "gradients/add_grad/Shape_1\n",
      "gradients/add_grad/BroadcastGradientArgs\n",
      "gradients/add_grad/Sum\n",
      "gradients/add_grad/Reshape\n",
      "gradients/add_grad/Sum_1\n",
      "gradients/add_grad/Reshape_1\n",
      "gradients/add_grad/tuple/group_deps\n",
      "gradients/add_grad/tuple/control_dependency\n",
      "gradients/add_grad/tuple/control_dependency_1\n",
      "gradients/MatMul_grad/MatMul\n",
      "gradients/MatMul_grad/MatMul_1\n",
      "gradients/MatMul_grad/tuple/group_deps\n",
      "gradients/MatMul_grad/tuple/control_dependency\n",
      "gradients/MatMul_grad/tuple/control_dependency_1\n",
      "beta1_power/Initializer/initial_value\n",
      "beta1_power\n",
      "beta1_power/IsInitialized/VarIsInitializedOp\n",
      "beta1_power/Assign\n",
      "beta1_power/Read/ReadVariableOp\n",
      "beta2_power/Initializer/initial_value\n",
      "beta2_power\n",
      "beta2_power/IsInitialized/VarIsInitializedOp\n",
      "beta2_power/Assign\n",
      "beta2_power/Read/ReadVariableOp\n",
      "W/Adam-op/Initializer/zeros/shape_as_tensor\n",
      "W/Adam-op/Initializer/zeros/Const\n",
      "W/Adam-op/Initializer/zeros\n",
      "W/Adam-op\n",
      "W/Adam-op/IsInitialized/VarIsInitializedOp\n",
      "W/Adam-op/Assign\n",
      "W/Adam-op/Read/ReadVariableOp\n",
      "W/Adam-op_1/Initializer/zeros/shape_as_tensor\n",
      "W/Adam-op_1/Initializer/zeros/Const\n",
      "W/Adam-op_1/Initializer/zeros\n",
      "W/Adam-op_1\n",
      "W/Adam-op_1/IsInitialized/VarIsInitializedOp\n",
      "W/Adam-op_1/Assign\n",
      "W/Adam-op_1/Read/ReadVariableOp\n",
      "b/Adam-op/Initializer/zeros\n",
      "b/Adam-op\n",
      "b/Adam-op/IsInitialized/VarIsInitializedOp\n",
      "b/Adam-op/Assign\n",
      "b/Adam-op/Read/ReadVariableOp\n",
      "b/Adam-op_1/Initializer/zeros\n",
      "b/Adam-op_1\n",
      "b/Adam-op_1/IsInitialized/VarIsInitializedOp\n",
      "b/Adam-op_1/Assign\n",
      "b/Adam-op_1/Read/ReadVariableOp\n",
      "Adam-op/learning_rate\n",
      "Adam-op/beta1\n",
      "Adam-op/beta2\n",
      "Adam-op/epsilon\n",
      "Adam-op/update_W/ResourceApplyAdam/ReadVariableOp\n",
      "Adam-op/update_W/ResourceApplyAdam/ReadVariableOp_1\n",
      "Adam-op/update_W/ResourceApplyAdam\n",
      "Adam-op/update_b/ResourceApplyAdam/ReadVariableOp\n",
      "Adam-op/update_b/ResourceApplyAdam/ReadVariableOp_1\n",
      "Adam-op/update_b/ResourceApplyAdam\n",
      "Adam-op/ReadVariableOp\n",
      "Adam-op/mul\n",
      "Adam-op/AssignVariableOp\n",
      "Adam-op/ReadVariableOp_1\n",
      "Adam-op/ReadVariableOp_2\n",
      "Adam-op/mul_1\n",
      "Adam-op/AssignVariableOp_1\n",
      "Adam-op/ReadVariableOp_3\n",
      "Adam-op\n",
      "ArgMax/dimension\n",
      "ArgMax\n",
      "ArgMax_1/dimension\n",
      "ArgMax_1\n",
      "correct_pred\n",
      "Cast\n",
      "Const_2\n",
      "accuracy\n",
      "save/filename/input\n",
      "save/filename\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Identity\n",
      "save/AssignVariableOp\n",
      "save/Identity_1\n",
      "save/AssignVariableOp_1\n",
      "save/Identity_2\n",
      "save/AssignVariableOp_2\n",
      "save/Identity_3\n",
      "save/AssignVariableOp_3\n",
      "save/Identity_4\n",
      "save/AssignVariableOp_4\n",
      "save/Identity_5\n",
      "save/AssignVariableOp_5\n",
      "save/Identity_6\n",
      "save/AssignVariableOp_6\n",
      "save/Identity_7\n",
      "save/AssignVariableOp_7\n",
      "save/restore_all\n",
      "init\n"
     ]
    }
   ],
   "source": [
    "# delete the current graph\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# import the graph from the file\n",
    "imported_graph = tf.compat.v1.train.import_meta_graph('linear_model.meta')\n",
    "\n",
    "# list all the tensors in the graph\n",
    "for tensor in tf.compat.v1.get_default_graph().get_operations():\n",
    "    print (tensor.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that I am interested in `loss` and `accuracy` of my model. We can easily get the values of corresponding tensors, providing the correct placeholders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Test loss: 0.32, test accuracy: 91.5%\n",
      "---------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run the session\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    # restore the saved vairable\n",
    "    imported_graph.restore(sess, './linear_model')\n",
    "        \n",
    "    # Accuracy\n",
    "    feed_dict_test = {'X:0': data.test.images, 'Y:0': data.test.labels}\n",
    "    loss_test, acc_test = sess.run(['loss:0', 'accuracy:0'], feed_dict=feed_dict_test)\n",
    "    print('---------------------------------------------------------')\n",
    "    print(\"Test loss: {0:.2f}, test accuracy: {1:.01%}\".format(loss_test, acc_test))\n",
    "    print('---------------------------------------------------------')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for reading! If you have any question or doubt, feel free to leave a comment in our [website](http://easy-tensorflow.com/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
