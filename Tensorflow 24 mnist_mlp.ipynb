{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple MLP with no hidden layer and 10 output neurons with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.examples.tutorials'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9528819780b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.examples.tutorials'"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "import sys\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-f5c407a462e3>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/gmanish/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/gmanish/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/gmanish/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/gmanish/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/gmanish/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(55000, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#you could also get data from the web, this way\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "#mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "dir=\"MNIST_data/\"\n",
    "# Import data\n",
    "mnist = input_data.read_data_sets(dir, one_hot=True)\n",
    "mnist.train.images.shape\n",
    "mnist.train.labels.shape\n",
    "mnist.test.images.shape\n",
    "mnist.test.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f39493a717c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mnist' is not defined"
     ]
    }
   ],
   "source": [
    "mnist.train.labels[0]\n",
    "mnist.train.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "def gen_image(arr):\n",
    "    two_d = (np.reshape(arr, (28, 28)) * 255).astype(np.uint8)\n",
    "    plt.imshow(two_d)\n",
    "    return plt\n",
    "\n",
    "#mnist.test.images[0]\n",
    "gen_image(mnist.test.images[0]).show()\n",
    "gen_image(mnist.test.images[1]).show()\n",
    "print(mnist.test.labels[0], mnist.test.labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-3ba2fc752236>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define loss and optimizer\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.22 s, sys: 750 ms, total: 1.97 s\n",
      "Wall time: 1.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train\n",
    "for _ in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, {x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(10)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(10)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "NameError",
     "evalue": "name 'mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b76687b6e19d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcorrect_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m print(sess.run(accuracy, feed_dict={x: mnist.test.images,\n\u001b[0m\u001b[1;32m      7\u001b[0m                                   y_: mnist.test.labels}))\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mnist' is not defined"
     ]
    }
   ],
   "source": [
    "y.shape\n",
    "y_.shape\n",
    "# Test trained model\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images,\n",
    "                                  y_: mnist.test.labels}))\n",
    "print(sess.run([W,b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP with multiple hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import math\n",
    "\n",
    "dir=\"MNIST_data/\"\n",
    "NUM_CLASSES = 10\n",
    "# The MNIST images are always 28x28 pixels.\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE\n",
    "batch_size=100\n",
    "hidden1=128\n",
    "hidden2=32\n",
    "learning_rate=0.01\n",
    "log_dir=\"logs/\"\n",
    "max_steps=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate placeholder variables to represent the input tensors.\n",
    "def placeholder_inputs(batch_size):\n",
    "  images_placeholder = tf.placeholder(tf.float32, shape=(batch_size, IMAGE_PIXELS))\n",
    "  labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "  return images_placeholder, labels_placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fills the feed_dict for training the given step with the next \"batch size\" examples\n",
    "def fill_feed_dict(data_set, images_pl, labels_pl):\n",
    "  images_feed, labels_feed = data_set.next_batch(batch_size)\n",
    "  feed_dict = {\n",
    "      images_pl: images_feed,\n",
    "      labels_pl: labels_feed,\n",
    "  }\n",
    "  return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs one evaluation against the full epoch of data.\n",
    "def do_eval(sess,eval_correct,images_placeholder,labels_placeholder,data_set):\n",
    "  true_count = 0  # Counts the number of correct predictions.\n",
    "  steps_per_epoch = data_set.num_examples // batch_size\n",
    "  num_examples = steps_per_epoch * batch_size\n",
    "  for step in range(steps_per_epoch):\n",
    "    feed_dict = fill_feed_dict(data_set,images_placeholder,labels_placeholder)\n",
    "    true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "  precision = float(true_count) / num_examples\n",
    "  print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n",
    "        (num_examples, true_count, precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the MNIST model\n",
    "def inference(images, hidden1_units, hidden2_units):\n",
    "  # Hidden 1\n",
    "  with tf.name_scope('hidden1'):\n",
    "    weights = tf.Variable(tf.truncated_normal([IMAGE_PIXELS, hidden1_units],\n",
    "                            stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))), name='weights')\n",
    "    biases = tf.Variable(tf.zeros([hidden1_units]), name='biases')\n",
    "    hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\n",
    "  # Hidden 2\n",
    "  with tf.name_scope('hidden2'):\n",
    "    weights = tf.Variable(tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "                            stddev=1.0 / math.sqrt(float(hidden1_units))), name='weights')\n",
    "    biases = tf.Variable(tf.zeros([hidden2_units]), name='biases')\n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "  # Linear\n",
    "  with tf.name_scope('softmax_linear'):\n",
    "    weights = tf.Variable(tf.truncated_normal([hidden2_units, NUM_CLASSES],\n",
    "                            stddev=1.0 / math.sqrt(float(hidden2_units))), name='weights')\n",
    "    biases = tf.Variable(tf.zeros([NUM_CLASSES]), name='biases')\n",
    "    logits = tf.matmul(hidden2, weights) + biases\n",
    "  return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates the loss from the logits and the labels.\n",
    "def lossFn(logits, labels):\n",
    "  labels = tf.to_int64(labels)\n",
    "  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='xentropy')\n",
    "  return tf.reduce_mean(cross_entropy, name='xentropy_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(loss, learning_rate):\n",
    "  # Add a scalar summary for the snapshot loss.\n",
    "  tf.summary.scalar('loss', loss)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  # Create a variable to track the global step.\n",
    "  global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "  train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "  return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the quality of the logits at predicting the label.\n",
    "def evaluation(logits, labels):\n",
    "  # For a classifier model, we can use the in_top_k Op.\n",
    "  # It returns a bool tensor with shape [batch_size] that is true for\n",
    "  # the examples where the label is in the top k (here k=1)\n",
    "  # of all logits for that example.\n",
    "  correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "  # Return the number of true entries.\n",
    "  return tf.reduce_sum(tf.cast(correct, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train MNIST for a number of steps\n",
    "def run_training():\n",
    "  # Get the sets of images and labels for training, validation, and\n",
    "  # test on MNIST.\n",
    "  data_sets = input_data.read_data_sets(dir)\n",
    "  with tf.Graph().as_default():\n",
    "    images_placeholder, labels_placeholder = placeholder_inputs(batch_size)\n",
    "    # Build a Graph that computes predictions from the inference model.\n",
    "    logits = inference(images_placeholder, hidden1, hidden2)\n",
    "    loss = lossFn(logits, labels_placeholder)\n",
    "    train_op = training(loss, learning_rate)\n",
    "    eval_correct = evaluation(logits, labels_placeholder)\n",
    "    \n",
    "    summary = tf.summary.merge_all()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "      start_time = time.time()\n",
    "      feed_dict = fill_feed_dict(data_sets.train, images_placeholder, labels_placeholder)\n",
    "      #The return values are the activations\n",
    "      # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "      # inspect the values of your Ops or variables, you may include them\n",
    "      # in the list passed to sess.run() and the value tensors will be\n",
    "      # returned in the tuple from the call.\n",
    "      _, loss_value = sess.run([train_op, loss],feed_dict=feed_dict)\n",
    "      duration = time.time() - start_time\n",
    "\n",
    "      # Write the summaries and print an overview fairly often.\n",
    "      if step % 100 == 0:\n",
    "        print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "        summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "        summary_writer.add_summary(summary_str, step)\n",
    "        summary_writer.flush()\n",
    "\n",
    "      # Save a checkpoint and evaluate the model periodically.\n",
    "      if (step + 1) % 1000 == 0 or (step + 1) == max_steps:\n",
    "        checkpoint_file = os.path.join(log_dir, 'model.ckpt')\n",
    "        saver.save(sess, checkpoint_file, global_step=step)\n",
    "        # Evaluate against the training set.\n",
    "        print('Training Data Eval:')\n",
    "        do_eval(sess,eval_correct,images_placeholder,labels_placeholder,data_sets.train)\n",
    "        # Evaluate against the validation set.\n",
    "        print('Validation Data Eval:')\n",
    "        do_eval(sess,eval_correct,images_placeholder,labels_placeholder,data_sets.validation)\n",
    "        # Evaluate against the test set.\n",
    "        print('Test Data Eval:')\n",
    "        do_eval(sess,eval_correct,images_placeholder,labels_placeholder,data_sets.test)\n",
    "    summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-15-531acd8dbc0e>:3: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Step 0: loss = 2.28 (0.249 sec)\n",
      "Step 100: loss = 2.12 (0.001 sec)\n",
      "Step 200: loss = 1.92 (0.001 sec)\n",
      "Step 300: loss = 1.58 (0.001 sec)\n",
      "Step 400: loss = 1.12 (0.001 sec)\n",
      "Step 500: loss = 0.90 (0.001 sec)\n",
      "Step 600: loss = 0.64 (0.001 sec)\n",
      "Step 700: loss = 0.64 (0.001 sec)\n",
      "Step 800: loss = 0.55 (0.001 sec)\n",
      "Step 900: loss = 0.60 (0.001 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 47693  Precision @ 1: 0.8671\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4380  Precision @ 1: 0.8760\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 8713  Precision @ 1: 0.8713\n",
      "Step 1000: loss = 0.50 (0.018 sec)\n",
      "Step 1100: loss = 0.56 (0.242 sec)\n",
      "Step 1200: loss = 0.47 (0.001 sec)\n",
      "Step 1300: loss = 0.49 (0.002 sec)\n",
      "Step 1400: loss = 0.51 (0.002 sec)\n",
      "Step 1500: loss = 0.44 (0.007 sec)\n",
      "Step 1600: loss = 0.30 (0.004 sec)\n",
      "Step 1700: loss = 0.38 (0.002 sec)\n",
      "Step 1800: loss = 0.36 (0.002 sec)\n",
      "Step 1900: loss = 0.31 (0.003 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 49259  Precision @ 1: 0.8956\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4525  Precision @ 1: 0.9050\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9017  Precision @ 1: 0.9017\n",
      "Step 2000: loss = 0.32 (0.041 sec)\n",
      "Step 2100: loss = 0.22 (0.006 sec)\n",
      "Step 2200: loss = 0.25 (0.485 sec)\n",
      "Step 2300: loss = 0.34 (0.003 sec)\n",
      "Step 2400: loss = 0.32 (0.002 sec)\n",
      "Step 2500: loss = 0.37 (0.001 sec)\n",
      "Step 2600: loss = 0.43 (0.001 sec)\n",
      "Step 2700: loss = 0.42 (0.001 sec)\n",
      "Step 2800: loss = 0.23 (0.001 sec)\n",
      "Step 2900: loss = 0.28 (0.002 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 49944  Precision @ 1: 0.9081\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4596  Precision @ 1: 0.9192\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9129  Precision @ 1: 0.9129\n",
      "Step 3000: loss = 0.21 (0.019 sec)\n",
      "Step 3100: loss = 0.32 (0.001 sec)\n",
      "Step 3200: loss = 0.31 (0.001 sec)\n",
      "Step 3300: loss = 0.30 (0.294 sec)\n",
      "Step 3400: loss = 0.29 (0.001 sec)\n",
      "Step 3500: loss = 0.25 (0.001 sec)\n",
      "Step 3600: loss = 0.24 (0.001 sec)\n",
      "Step 3700: loss = 0.32 (0.001 sec)\n",
      "Step 3800: loss = 0.28 (0.001 sec)\n",
      "Step 3900: loss = 0.23 (0.001 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 50556  Precision @ 1: 0.9192\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4627  Precision @ 1: 0.9254\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9226  Precision @ 1: 0.9226\n",
      "Step 4000: loss = 0.30 (0.017 sec)\n",
      "Step 4100: loss = 0.23 (0.001 sec)\n",
      "Step 4200: loss = 0.36 (0.001 sec)\n",
      "Step 4300: loss = 0.15 (0.002 sec)\n",
      "Step 4400: loss = 0.21 (0.274 sec)\n",
      "Step 4500: loss = 0.25 (0.001 sec)\n",
      "Step 4600: loss = 0.36 (0.001 sec)\n",
      "Step 4700: loss = 0.21 (0.001 sec)\n",
      "Step 4800: loss = 0.30 (0.001 sec)\n",
      "Step 4900: loss = 0.19 (0.001 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 50933  Precision @ 1: 0.9261\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4668  Precision @ 1: 0.9336\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9275  Precision @ 1: 0.9275\n"
     ]
    }
   ],
   "source": [
    "import shutil,os\n",
    "#if os.path.exists(log_dir):\n",
    "#    shutil.rmtree(log_dir)\n",
    "tf.gfile.MakeDirs(log_dir)\n",
    "run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable:0 (784, 10)\n",
      "2\n",
      "784\n",
      "10\n",
      "7840\n",
      "Variable_1:0 (10,)\n",
      "1\n",
      "10\n",
      "10\n",
      "7850\n"
     ]
    }
   ],
   "source": [
    "#Count total number of parameters\n",
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    print(variable.name, shape)\n",
    "    print(len(shape))\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        print(dim)\n",
    "        variable_parameters *= dim.value\n",
    "    print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
